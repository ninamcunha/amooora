{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a4baf5",
   "metadata": {},
   "source": [
    "# Amooora Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dccf46",
   "metadata": {},
   "source": [
    "Nesse notebook vamos explorar o modelo dispon√≠vel no Huggingface [bert-base-therapist-topic-classification-eng](https://huggingface.co/AIPsy/bert-base-therapist-topic-classification-eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9fe536",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'Time Up and Future Meetings',\n",
    "            1: 'Complex Emotions Toward Him',\n",
    "            2: 'Desires and Disappointments',\n",
    "            3: 'Personal Growth and Decision-Making',\n",
    "            4: 'Self-Acceptance and Relationships',\n",
    "            5: 'Understanding and Confronting Fear',\n",
    "            6: 'See and Understanding Conversations',\n",
    "            7: 'Clarifying Meaning and Intent',\n",
    "            8: 'Desire to Escape and Leave',\n",
    "            9: 'Uncertainty and Understanding Issues',\n",
    "            10: 'Open Conversation and Sharing',\n",
    "            11: 'Exploring Emotional Hurt and Bitterness',\n",
    "            12: 'Guilt and Self-Blame Dynamics',\n",
    "            13: 'Dynamics of Meaningful Relationships',\n",
    "            14: 'Struggles and Desires in Learning',\n",
    "            15: 'Gender Roles and Relationships',\n",
    "            16: 'Struggles with Personal Change',\n",
    "            17: 'Complex Mother-Sibling Relationships',\n",
    "            18: 'Voices and Perception of Sound',\n",
    "            19: 'Difficulties and Emotional Burdens',\n",
    "            20: 'Fear and Reflection on Aging',\n",
    "            21: 'Emotions of Crying and Tears',\n",
    "            22: 'Father-Child Relationships and Authority',\n",
    "            23: 'Possibilities and Potential Outcomes',\n",
    "            24: 'Inner Struggle and Helplessness',\n",
    "            25: 'Pursuing Meaningful Personal Goals',\n",
    "            26: 'Job Anxiety and Self-Reflection',\n",
    "            27: 'Marriage Anxiety and Dependence',\n",
    "            28: 'Expressions of Anger and Frustration',\n",
    "            29: 'Nurturing the inner child',\n",
    "            30: 'Therapy and Father Relationships',\n",
    "            31: 'Expressions of Happiness and Joy',\n",
    "            32: 'Revisiting the Past Together',\n",
    "            33: 'Drinking Habits and Concerns',\n",
    "            34: 'Managing and Increasing Energy Levels',\n",
    "            35: 'Safety and Self-Protection Strategies',\n",
    "            36: 'Understanding Depression and Its Roots'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce18096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dedefla/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9932514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"AIPsy/bert-base-therapist-topic-classification-eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eacc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b707123d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_preprocessed</th>\n",
       "      <th>answered_percent</th>\n",
       "      <th>text_length</th>\n",
       "      <th>topic_0_from_five</th>\n",
       "      <th>topic_1_from_five</th>\n",
       "      <th>topic_2_from_five</th>\n",
       "      <th>topic_3_from_five</th>\n",
       "      <th>topic_4_from_five</th>\n",
       "      <th>topic_0_from_two</th>\n",
       "      <th>topic_1_from_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would love think kind intellectual either dumb...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1565</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.023574</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.023509</td>\n",
       "      <td>0.271588</td>\n",
       "      <td>0.728412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chef mean workaholic love cook regardless whet...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>815</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>0.339134</td>\n",
       "      <td>0.027590</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.027468</td>\n",
       "      <td>0.446532</td>\n",
       "      <td>0.553468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im ashamed much write public text online date ...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3728</td>\n",
       "      <td>0.914891</td>\n",
       "      <td>0.021284</td>\n",
       "      <td>0.021307</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.021311</td>\n",
       "      <td>0.280111</td>\n",
       "      <td>0.719889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>work library go school read thing write old de...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>330</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.045620</td>\n",
       "      <td>0.817402</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>0.045524</td>\n",
       "      <td>0.330528</td>\n",
       "      <td>0.669472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey hows go currently vague profile know come ...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>496</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>0.872147</td>\n",
       "      <td>0.032119</td>\n",
       "      <td>0.031812</td>\n",
       "      <td>0.450953</td>\n",
       "      <td>0.549047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               combined_preprocessed  answered_percent  \\\n",
       "0  would love think kind intellectual either dumb...             100.0   \n",
       "1  chef mean workaholic love cook regardless whet...              60.0   \n",
       "2  im ashamed much write public text online date ...              90.0   \n",
       "3  work library go school read thing write old de...              70.0   \n",
       "4  hey hows go currently vague profile know come ...              50.0   \n",
       "\n",
       "   text_length  topic_0_from_five  topic_1_from_five  topic_2_from_five  \\\n",
       "0         1565           0.905500           0.023574           0.023639   \n",
       "1          815           0.027683           0.339134           0.027590   \n",
       "2         3728           0.914891           0.021284           0.021307   \n",
       "3          330           0.045781           0.045620           0.817402   \n",
       "4          496           0.031946           0.031976           0.872147   \n",
       "\n",
       "   topic_3_from_five  topic_4_from_five  topic_0_from_two  topic_1_from_two  \n",
       "0           0.023778           0.023509          0.271588          0.728412  \n",
       "1           0.578125           0.027468          0.446532          0.553468  \n",
       "2           0.021207           0.021311          0.280111          0.719889  \n",
       "3           0.045673           0.045524          0.330528          0.669472  \n",
       "4           0.032119           0.031812          0.450953          0.549047  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../raw_data/text_and_topics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8798d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would love think kind intellectual either dumbest smart guy smartest dumb guy cant say tell difference love talk idea concept forge odd metaphor instead recite cliche like simularities friend mine house underwater salt mine favorite word salt way weird choice know thing life better metaphor seek make little better everyday productively lazy way get tire tie shoe consider hire five year old would probably tie shoe decide wear leather shoe dress shoe love really serious really deep conversation really silly stuff will snap light hearted rant kiss dont funny able make laugh able bend spoon mind telepathically make smile still work love life cool let wind blow extra point read guess favorite video game hint give yet lastly good attention span currently work international agent freight forward company import export domestic know work online class try better free time perhaps hour worth good book video game lazy sunday make people laugh rant good salt find simplicity complexity complexity simplicity way look six foot half asian half caucasian mutt make tough notice blend book absurdistan republic mouse men book make want cry catcher rye prince movie gladiator operation valkyrie producer periscope show borgia arrest development game throne monty python music aesop rock hail mary mallon george thorogood delaware destroyer felt food im anything food water cell phone shelter duality humorous thing try find someone hang anything except club new california look someone wisper secret want sweep foot tire norm want catch coffee bite want talk philosophy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.iloc[0]\n",
    "text.combined_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "558ea12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.44941967725753784}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text.combined_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40c68059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id like think im pretty normal guy im extraordinary way love ordinary get bad rap opinion life really simple honestly interest doesnt take much make happy im designer basically mean give lot think care thing look relationship also mean im visual person love movie love everything film still love experience go watch old theatre hate like old film foreign film interpret snobby feel like there many great movie havent see yet im really older stuff im also new release love travel try get u often possible enjoy break daily routine element force think think everything take grant may spend monthlong sabbatical japan ive also thailand vietnam scotland briefly various spot around europe want healthy seem like important thing since hit milestone birthday ive try change lifestyle behave health consciously actually eat breakfast still hard time one eat veggie less meat love pork run nearly much get bed earlier love sleep try right thing try harm observe learn lot pay attention clean always try leave thing better find organize anything everything maybe dont look age wait minute people wouldnt notice ive tell hmmm ive tell look serious guess thats natural facial expression curse book love murakami really need branch though ive read author coursemostly classic id like read movie love kind especially stuff pta aronofsky coppola fellini fincher godard hitchcock kubrick kurosawa malick nolan ozu truffaut wkw music long make true feel genre doesnt matter classical electronic hiphop jazz pop rock love mix variety bach beach boy beasties beatles beethoven black key ray charles chopin nat king cole coltrane cream dangelo mile davis debussy de la delerue door bob dylan bill evans stan getz al green erroll garner gorillaz king leon kink liszt delico lovin spoonful massive attack mozart phoenix pixy police prince queen radiohead otis redding prt pepper schubert stone spoon stroke talk head three dog night tribe turtle wilco id like start go show food im hardcore foodie definitely appreciate good meal dont think good food expensive fancy restaurant eat kind tend like asian cuisine chinese japanese thai vietnamese enjoy try new food discover good restaurant jellybeangummy good music good food passport movie photography smile friendly outgo id like get outdoors often active dinner friend relax home movie long week youre curious'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[234].combined_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21323c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(df.iloc[234].combined_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33f326e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_31', 'score': 0.49363023042678833}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7b964d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expressions of Happiness and Joy'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[int(result[0]['label'].split('_')[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "313b37d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.combined_preprocessed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cdf8310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df.combined_preprocessed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef5e187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/base.py:1352\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1355\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1356\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36mPipeline.run_multi\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_multi\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(item, preprocess_params, forward_params, postprocess_params) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_multi\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1374\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1376\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:183\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2877\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2876\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2877\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/amooora/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2937\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2934\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2939\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2940\u001b[0m     )\n\u001b[1;32m   2942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2946\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "pipe(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd25b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
